---
layout:     post                    
title:      深度学习学习笔记9-深度卷积神经网络AlexNet            
subtitle:   Dive into deep learning 
date:       2020-10-26              
author:     Resulte                      
header-img: img/post-bg-universe.jpg 
catalog: true                       
tags:                               
    - 深度学习

---

# 深度学习学习笔记9-深度卷积神经网络AlexNet 

### AlexNet模型

2012年，AlexNet横空出世。这个模型的名字来源于论文第一作者的姓名Alex Krizhevsky。AlexNet使用了8层卷积神经网络，并以很大的优势赢得了ImageNet 2012图像识别挑战赛。它首次证明了学习到的特征可以超越手工设计的特征，从而一举打破计算机视觉研究的前状。

AlexNet与LeNet的设计理念非常相似，但也有显著的区别。

第一，与相对较小的LeNet相比，AlexNet包含8层变换，其中有5层卷积和2层全连接隐藏层，以及1个全连接输出层。下面我们来详细描述这些层的设计。

AlexNet第一层中的卷积窗口形状是11×11。因为ImageNet中绝大多数图像的高和宽均比MNIST图像的高和宽大10倍以上，ImageNet图像的物体占用更多的像素，所以需要更大的卷积窗口来捕获物体。第二层中的卷积窗口形状减小到5×5，之后全采用3×3。此外，第一、第二和第五个卷积层之后都使用了窗口形状为3×3、步幅为2的最大池化层。而且，AlexNet使用的卷积通道数也大于LeNet中的卷积通道数数十倍。

紧接着最后一个卷积层的是两个输出个数为4,096的全连接层。这两个巨大的全连接层带来将近1 GB的模型参数。由于早期显存的限制，最早的AlexNet使用双数据流的设计使一块GPU只需要处理一半模型。幸运的是，显存在过去几年得到了长足的发展，因此通常我们不再需要这样的特别设计了。

第二，AlexNet将sigmoid激活函数改成了更加简单的ReLU激活函数。一方面，ReLU激活函数的计算更简单，例如它并没有sigmoid激活函数中的求幂运算。另一方面，ReLU激活函数在不同的参数初始化方法下使模型更容易训练。这是由于当sigmoid激活函数输出极接近0或1时，这些区域的梯度几乎为0，从而造成反向传播无法继续更新部分模型参数；而ReLU激活函数在正区间的梯度恒为1。因此，若模型参数初始化不当，sigmoid函数可能在正区间得到几乎为0的梯度，从而令模型无法得到有效训练。

第三，AlexNet通过丢弃法来控制全连接层的模型复杂度。而LeNet并没有使用丢弃法。

第四，AlexNet引入了大量的图像增广，如翻转、裁剪和颜色变化，从而进一步扩大数据集来缓解过拟合。

**AlexNet之所以能够成功，跟这个模型设计的特点有关，主要有：**

- 使用了非线性激活函数：ReLU
- 防止过拟合的方法：Dropout，数据扩充（Data augmentation）
- 其他：多GPU实现，LRN归一化层的使用

**1、使用ReLU激活函数**

传统的神经网络普遍使用Sigmoid或者tanh等非线性函数作为激励函数，然而它们容易出现梯度弥散或梯度饱和的情况。以Sigmoid函数为例，当输入的值非常大或者非常小的时候，这些神经元的梯度接近于0（梯度饱和现象），如果输入的初始值很大的话，梯度在反向传播时因为需要乘上一个Sigmoid导数，会造成梯度越来越小，导致网络变的很难学习。

在AlexNet中，使用了ReLU （Rectified Linear Units）激励函数，使用ReLU替代Sigmoid/tanh，由于ReLU是线性的，且导数始终为1，计算量大大减少，收敛速度会比Sigmoid/tanh快很多.

**2、数据扩充（Data augmentation）**

有一种观点认为神经网络是靠数据喂出来的，如果能够增加训练数据，提供海量数据进行训练，则能够有效提升算法的准确率，因为这样可以避免过拟合，从而可以进一步增大、加深网络结构。而当训练数据有限时，可以通过一些变换从已有的训练数据集中生成一些新的数据，以快速地扩充训练数据。
其中，最简单、通用的图像数据变形的方式：水平翻转图像，从原始图像中随机裁剪、平移变换，颜色、光照变换.

AlexNet在训练时，在数据扩充（data augmentation）这样处理：

（1）随机裁剪，对256×256的图片进行随机裁剪到224×224，然后进行水平翻转，相当于将样本数量增加了（（256-224）^2）×2=2048倍；

（2）测试的时候，对左上、右上、左下、右下、中间分别做了5次裁剪，然后翻转，共10个裁剪，之后对结果求平均。作者说，如果不做随机裁剪，大网络基本上都过拟合；

（3）对RGB空间做PCA（主成分分析），然后对主成分做一个（0, 0.1）的高斯扰动，也就是对颜色、光照作变换，结果使错误率又下降了1%。

**3、重叠池化 (Overlapping Pooling)**

一般的池化（Pooling）是不重叠的，池化区域的窗口大小与步长相同。在AlexNet中使用的池化（Pooling）却是可重叠的，也就是说，在池化的时候，每次移动的步长小于池化的窗口长度。AlexNet池化的大小为3×3的正方形，每次池化移动步长为2，这样就会出现重叠。重叠池化可以避免过拟合，这个策略贡献了0.3%的Top-5错误率。

**4、局部归一化（Local Response Normalization，简称LRN）**

在神经生物学有一个概念叫做“侧抑制”（lateral inhibitio），指的是被激活的神经元抑制相邻神经元。归一化（normalization）的目的是“抑制”，局部归一化就是借鉴了“侧抑制”的思想来实现局部抑制，尤其当使用ReLU时这种“侧抑制”很管用，因为ReLU的响应结果是无界的（可以非常大），所以需要归一化。使用局部归一化的方案有助于增加泛化能力。

LRN的核心思想就是利用临近的数据做归一化。

**5、Dropout**

引入Dropout主要是为了防止过拟合。在神经网络中Dropout通过修改神经网络本身结构来实现，对于某一层的神经元，通过定义的概率将神经元置为0，这个神经元就不参与前向和后向传播，就如同在网络中被删除了一样，同时保持输入层与输出层神经元的个数不变，然后按照神经网络的学习方法进行参数更新。在下一次迭代中，又重新随机删除一些神经元（置为0），直至训练结束。

Dropout应该算是AlexNet中一个很大的创新，以至于“神经网络之父”Hinton在后来很长一段时间里的演讲中都拿Dropout说事。Dropout也可以看成是一种模型组合，每次生成的网络结构都不一样，通过组合多个模型的方式能够有效地减少过拟合，Dropout只需要两倍的训练时间即可实现模型组合（类似取平均）的效果，非常高效。

**6、多GPU训练**

AlexNet当时使用了GTX580的GPU进行训练，由于单个GTX 580 GPU只有3GB内存，这限制了在其上训练的网络的最大规模，因此他们在每个GPU中放置一半核（或神经元），将网络分布在两个GPU上进行并行计算，大大加快了AlexNet的训练速度。