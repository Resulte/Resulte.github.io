---
layout:     post                    
title:      深度学习学习笔记4-多层感知机              
subtitle:   Dive into deep learning 
date:       2020-10-18              
author:     Resulte                      
header-img: img/post-bg-universe.jpg 
catalog: true                       
tags:                               
    - 深度学习

---

# 深度学习学习笔记4-多层感知机

### 一、简介

多层感知机在单层神经网络的基础上引入了一到多个隐藏层（hidden layer）。隐藏层位于输入层和输出层之间。

隐藏层中的神经元和输入层中各个输入完全连接，输出层中的神经元和隐藏层中的各个神经元也完全连接。因此，多层感知机中的隐藏层和输出层都是全连接层。

我们先来看一种含单隐藏层的多层感知机的设计。其输出**O**的计算为

![1](https://edu-boker.oss-cn-beijing.aliyuncs.com/dl/3/1.jpg)

也就是将隐藏层的输出直接作为输出层的输入。如果将以上两个式子联立起来，可以得到

![2](https://edu-boker.oss-cn-beijing.aliyuncs.com/dl/3/2.jpg)

从联立后的式子可以看出，虽然神经网络引入了隐藏层，却依然等价于一个单层神经网络：其中输出层权重参数为**W**h**W**o，偏差参数为**b**h**W**o+**b**o。不难发现，即便再添加更多的隐藏层，以上设计依然只能与仅含输出层的单层神经网络等价。

### 二、激活函数

上述问题的根源在于全连接层只是对数据做仿射变换（affine transformation），而多个仿射变换的叠加仍然是一个仿射变换。解决问题的一个方法是引入非线性变换，例如对隐藏变量使用按元素运算的非线性函数进行变换，然后再作为下一个全连接层的输入。这个非线性函数被称为激活函数（activation function）。下面我们介绍几个常用的激活函数。

- ### ReLU函数

ReLU（rectified linear unit）函数提供了一个很简单的非线性变换。给定元素xx，该函数定义为

![3](https://edu-boker.oss-cn-beijing.aliyuncs.com/dl/3/3.jpg)

可以看出，ReLU函数只保留正数元素，并将负数元素清零。

![4](https://edu-boker.oss-cn-beijing.aliyuncs.com/dl/3/4.jpg)

- ### sigmoid函数

sigmoid函数可以将元素的值变换到0和1之间：

![5](https://edu-boker.oss-cn-beijing.aliyuncs.com/dl/3/5.jpg)

sigmoid函数在早期的神经网络中较为普遍，但它目前逐渐被更简单的ReLU函数取代。在循环神经网络会利用它值域在0到1之间这一特性来控制信息在神经网络中的流动。下面绘制了sigmoid函数。当输入接近0时，sigmoid函数接近线性变换。

![6](https://edu-boker.oss-cn-beijing.aliyuncs.com/dl/3/6.jpg)

- ### tanh函数

tanh（双曲正切）函数可以将元素的值变换到-1和1之间：

![7](https://edu-boker.oss-cn-beijing.aliyuncs.com/dl/3/7.jpg)

当输入接近0时，tanh函数接近线性变换。虽然该函数的形状和sigmoid函数的形状很像，但tanh函数在坐标系的原点上对称。

![8](https://edu-boker.oss-cn-beijing.aliyuncs.com/dl/3/8.jpg)

### 三、多层感知机

多层感知机（multilayer perceptron，MLP）就是含有至少一个隐藏层的由全连接层组成的神经网络，且每个隐藏层的输出通过激活函数进行变换。多层感知机的层数和各隐藏层中隐藏单元个数都是超参数。以单隐藏层为例，多层感知机按以下方式计算输出：

![9](https://edu-boker.oss-cn-beijing.aliyuncs.com/dl/3/9.jpg)

其中ϕ表示激活函数。在分类问题中，我们可以对输出**O**做softmax运算，并使用softmax回归中的交叉熵损失函数。 在回归问题中，我们将输出层的输出个数设为1，并将输出**O**直接提供给线性回归中使用的平方损失函数。