---
layout:     post                    
title:      深度学习学习笔记3-softmax回归              
subtitle:   Dive into deep learning 
date:       2020-10-16              
author:     Resulte                      
header-img: img/post-bg-universe.jpg 
catalog: true                       
tags:                               
    - 深度学习

---

# 深度学习学习笔记3-softmax回归

### 一、简介

线性回归模型适用于输出为连续值的情景。在另一类情景中，模型输出可以是一个像图像类别这样的离散值。对于这样的离散值预测问题，我们可以使用诸如softmax回归在内的分类模型。和线性回归不同，softmax回归的输出单元从一个变成了多个，且引入了softmax运算使输出更适合离散值的预测和训练。

线性回归和softmax回归都是单层神经网络。

### 二、例子

让我们考虑一个简单的图像分类问题，其输入图像的高和宽均为2像素，且色彩为灰度。这样每个像素值都可以用一个标量表示。我们将图像中的4像素分别记为x1,x2,x3,x4。假设训练数据集中图像的真实标签为狗、猫或鸡（假设可以用4像素表示出这3种动物），这些标签分别对应离散值y1,y2,y3。

我们通常使用离散的数值来表示类别，例如y1=1,y2=2,y3=3。如此，一张图像的标签为1、2和3这3个数值中的一个。虽然我们仍然可以使用回归模型来进行建模，并将预测值就近定点化到1、2和3这3个离散值之一，但这种连续值到离散值的转化通常会影响到分类质量。因此我们一般使用更加适合离散值输出的模型来解决分类问题。

### 三、模型

softmax回归跟线性回归一样将输入特征与权重做线性叠加。与线性回归的一个主要不同在于，softmax回归的输出值个数等于标签里的类别数。因为一共有4种特征和3种输出动物类别，所以权重包含12个标量（带下标的w）、偏差包含3个标量（带下标的b），且对每个输入计算o1,o2,o3这3个输出：

o1=x1w11+x2w21+x3w31+x4w41+b1,
o2=x1w12+x2w22+x3w32+x4w42+b2,
o3=x1w13+x2w23+x3w33+x4w43+b3.

### 四、softmax运算

既然分类问题需要得到离散的预测输出，一个简单的办法是将输出值oi当作预测类别是ii的置信度，并将值最大的输出所对应的类作为预测输出。例如，如果o1,o2,o3分别为0.1,10,0.1，由于o2最大，那么预测类别为2，其代表猫。

然而，直接使用输出层的输出有两个问题。一方面，由于输出层的输出值的范围不确定，我们难以直观上判断这些值的意义。例如，刚才举的例子中的输出值10表示“很置信”图像类别为猫，因为该输出值是其他两类的输出值的100倍。但如果o1=o3=10^3，那么输出值10却又表示图像类别为猫的概率很低。另一方面，由于真实标签是离散值，这些离散值与不确定范围的输出值之间的误差难以衡量。

softmax运算符（softmax operator）解决了以上两个问题。它通过下式将输出值变换成值为正且和为1的概率分布：

​	![1](https://edu-boker.oss-cn-beijing.aliyuncs.com/dl/2/1.jpg)

容易看出ŷ 1+ŷ 2+ŷ 3=1且0≤ŷ 1,ŷ 2,ŷ 3≤1，因此ŷ 1,ŷ 2,ŷ 3是一个合法的概率分布。这时候，如果ŷ 2=0.8，不管ŷ 1和ŷ 3的值是多少，我们都知道图像类别为猫的概率是80%。因此softmax运算不改变预测类别输出。

### 五、交叉熵损失函数

前面提到，使用softmax运算后可以更方便地与离散标签计算误差。我们已经知道，softmax运算将输出变换成一个合法的类别预测分布。实际上，真实标签也可以用类别分布表达：对于样本i，使其第y(i)（样本i类别的离散数值）个元素为1，其余为0。这样我们的训练目标可以设为使预测概率分布尽可能接近真实的标签概率分布。

我们可以像线性回归那样使用平方损失函数。然而，想要预测分类结果正确，我们其实并不需要预测概率完全等于标签概率。例如，在图像分类的例子里，如果y(i)=3，那么我们只需要ŷ (i)3比其他两个预测值ŷ (i)1和ŷ (i)2大就行了。即使ŷ (i)3值为0.6，不管其他两个预测值为多少，类别预测均正确。而平方损失则过于严格，例如ŷ (i)1=ŷ (i)2=0.2比ŷ (i)1=0,ŷ (i)2=0.4的损失要小很多，虽然两者都有同样正确的分类预测结果。

改善上述问题的一个方法是使用更适合衡量两个概率分布差异的测量函数。其中，交叉熵（cross entropy）是一个常用的衡量方法:

![2](https://edu-boker.oss-cn-beijing.aliyuncs.com/dl/2/2.jpg)

假设训练数据集的样本数为nn，交叉熵损失函数定义为：

![3](https://edu-boker.oss-cn-beijing.aliyuncs.com/dl/2/3.jpg)

也就是说，交叉熵只关心对正确类别的预测概率，因为只要其值足够大，就可以确保分类结果正确。

### 六、模型预测及评价

在训练好softmax回归模型后，给定任一样本特征，就可以预测每个输出类别的概率。通常，我们把预测概率最大的类别作为输出类别。如果它与真实类别（标签）一致，说明这次预测是正确的。例如我们可以使用准确率（accuracy）来评价模型的表现。它等于正确预测数量与总预测数量之比。