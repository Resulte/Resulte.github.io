---
layout:     post                    
title:      深度学习学习笔记3-softmax回归              
subtitle:   Dive into deep learning 
date:       2020-10-16              
author:     Resulte                      
header-img: img/post-bg-universe.jpg 
catalog: true                       
tags:                               
    - 深度学习

---

# 深度学习学习笔记3-softmax回归

### 一、简介

线性回归模型适用于输出为连续值的情景。在另一类情景中，模型输出可以是一个像图像类别这样的离散值。对于这样的离散值预测问题，我们可以使用诸如softmax回归在内的分类模型。和线性回归不同，softmax回归的输出单元从一个变成了多个，且引入了softmax运算使输出更适合离散值的预测和训练。

线性回归和softmax回归都是单层神经网络。

### 二、例子

让我们考虑一个简单的图像分类问题，其输入图像的高和宽均为2像素，且色彩为灰度。这样每个像素值都可以用一个标量表示。我们将图像中的4像素分别记为x1,x2,x3,x4。假设训练数据集中图像的真实标签为狗、猫或鸡（假设可以用4像素表示出这3种动物），这些标签分别对应离散值y1,y2,y3。

我们通常使用离散的数值来表示类别，例如y1=1,y2=2,y3=3。如此，一张图像的标签为1、2和3这3个数值中的一个。虽然我们仍然可以使用回归模型来进行建模，并将预测值就近定点化到1、2和3这3个离散值之一，但这种连续值到离散值的转化通常会影响到分类质量。因此我们一般使用更加适合离散值输出的模型来解决分类问题。

### 三、模型

softmax回归跟线性回归一样将输入特征与权重做线性叠加。与线性回归的一个主要不同在于，softmax回归的输出值个数等于标签里的类别数。因为一共有4种特征和3种输出动物类别，所以权重包含12个标量（带下标的w）、偏差包含3个标量（带下标的b），且对每个输入计算o1,o2,o3这3个输出：

o1=x1w11+x2w21+x3w31+x4w41+b1,
o2=x1w12+x2w22+x3w32+x4w42+b2,
o3=x1w13+x2w23+x3w33+x4w43+b3.

### 四、模型训练

接下来我们需要通过数据来寻找特定的模型参数值，使模型在数据上的误差尽可能小。这个过程叫作模型训练（model training）。下面我们介绍模型训练所涉及的3个要素。

- **训练数据**

我们通常收集一系列的真实数据，例如多栋房屋的真实售出价格和它们对应的面积和房龄。我们希望在这个数据上面寻找模型参数来使模型的预测价格与真实价格的误差最小。在机器学习术语里，该数据集被称为训练数据集（training data set）或训练集（training set），一栋房屋被称为一个样本（sample），其真实售出价格叫作标签（label），用来预测标签的两个因素叫作特征（feature）。特征用来表征样本的特点。

假设我们采集的样本数为n，索引为i的样本的特征为x(i)1和x(i)2，标签为y(i)。对于索引为i的房屋，线性回归模型的房屋价格预测表达式为
$$
ŷ (i)=x(i)1w1+x(i)2w2+b.
$$

- **损失函数**

在模型训练中，我们需要衡量价格预测值与真实值之间的误差。在机器学习里，将衡量误差的函数称为损失函数（loss function）。这里使用的平方误差函数也称为平方损失（square loss）:
$$
ℓ(i)(w1,w2,b)=1/2(ŷ (i)−y(i))^2,
$$
通常，我们用训练数据集中所有样本误差的平均来衡量模型预测的质量.

在模型训练中，我们希望找出一组模型参数，来使训练样本平均损失最小.

- **优化算法**

当模型和损失函数形式较为简单时，上面的误差最小化问题的解可以直接用公式表达出来。这类解叫作解析解（analytical solution）。本节使用的线性回归和平方误差刚好属于这个范畴。然而，大多数深度学习模型并没有解析解，只能通过优化算法有限次迭代模型参数来尽可能降低损失函数的值。这类解叫作数值解（numerical solution）。

在求数值解的优化算法中，小批量随机梯度下降（mini-batch stochastic gradient descent）在深度学习中被广泛使用。它的算法很简单：先选取一组模型参数的初始值，如随机选取；接下来对参数进行多次迭代，使每次迭代都可能降低损失函数的值。在每次迭代中，先随机均匀采样一个由固定数目训练数据样本所组成的小批量（mini-batch）B，然后求小批量中数据样本的平均损失有关模型参数的导数（梯度），最后用此结果与预先设定的一个正数的乘积作为模型参数在本次迭代的减小量。

批量大小和学习率的值是人为设定的，并不是通过模型训练学出的，因此叫作超参数（hyperparameter）。我们通常所说的“调参”指的正是调节超参数，例如通过反复试错来找到超参数合适的值。在少数情况下，超参数也可以通过模型训练学出。

### 五、模型预测

模型训练完成后，我们将模型参数w1,w2,b在优化算法停止时的值分别记作ŵ 1,ŵ 2,b̂ 。注意，这里我们得到的并不一定是最小化损失函数的最优解，而是对最优解的一个近似。然后，我们就可以使用学出的线性回归模型x1ŵ 1+x2ŵ 2+b̂ 来估算训练数据集以外任意一栋面积（平方米）为x1、房龄（年）为x2的房屋的价格了。这里的估算也叫作模型预测、模型推断或模型测试。