---
layout:     post                    
title:      深度学习学习笔记7-卷积神经网络             
subtitle:   Dive into deep learning 
date:       2020-10-24              
author:     Resulte                      
header-img: img/post-bg-universe.jpg 
catalog: true                       
tags:                               
    - 深度学习

---

# 深度学习学习笔记7-卷积神经网络

卷积神经网络（convolutional neural network）是含有卷积层（convolutional layer）的神经网络。本章中介绍的卷积神经网络均使用最常见的二维卷积层。它有高和宽两个空间维度，常用来处理图像数据。

### 一、二维卷积层

二维卷积层将输入和卷积核做互相关运算，并加上一个标量偏差来得到输出。卷积层的模型参数包括了卷积核和标量偏差。在训练模型的时候，通常我们先对卷积核随机初始化，然后不断迭代卷积核和偏差。

**二维互相关运算**

虽然卷积层得名于卷积（convolution）运算，但我们通常在卷积层中使用更加直观的互相关（cross-correlation）运算。在二维卷积层中，一个二维输入数组和一个二维核（kernel）数组通过互相关运算输出一个二维数组。 我们用一个具体例子来解释二维互相关运算的含义。如下图所示，输入是一个高和宽均为3的二维数组。我们将该数组的形状记为3×3或（3，3）。核数组的高和宽分别为2。该数组在卷积计算中又称卷积核或过滤器（filter）。卷积核窗口（又称卷积窗口）的形状取决于卷积核的高和宽，即2×2。图中的阴影部分为第一个输出元素及其计算所使用的输入和核数组元素：0×0+1×1+3×2+4×3=19。

<img src="https://edu-boker.oss-cn-beijing.aliyuncs.com/dl/6/1.png" style="zoom:50%;" />

在二维互相关运算中，卷积窗口从输入数组的最左上方开始，按从左往右、从上往下的顺序，依次在输入数组上滑动。当卷积窗口滑动到某一位置时，窗口中的输入子数组与核数组按元素相乘并求和，得到输出数组中相应位置的元素。

### 二、互相关运算和卷积运算

实际上，卷积运算与互相关运算类似。为了得到卷积运算的输出，我们只需将核数组左右翻转并上下翻转，再与输入数组做互相关运算。可见，卷积运算和互相关运算虽然类似，但如果它们使用相同的核数组，对于同一个输入，输出往往并不相同。

那么，你也许会好奇卷积层为何能使用互相关运算替代卷积运算。其实，在深度学习中核数组都是学出来的：卷积层无论使用互相关运算或卷积运算都不影响模型预测时的输出。

### 三、特征图和感受野

二维卷积层输出的二维数组可以看作输入在空间维度（宽和高）上某一级的表征，也叫特征图（feature map）。影响元素x的前向计算的所有可能输入区域（可能大于输入的实际尺寸）叫做x的感受野（receptive field）。以上图为例，输入中阴影部分的4个元素是输出中阴影部分元素的感受野。我们将上图中形状为2×2的输出记为Y，并考虑一个更深的卷积神经网络：将Y与另一个形状为2×2的核数组做互相关运算，输出单个元素z。那么，z在Y上的感受野包括Y的全部4个元素，在输入上的感受野包括其中全部9个元素。可见，我们可以通过更深的卷积神经网络使特征图中单个元素的感受野变得更加广阔，从而捕捉输入上更大尺寸的特征。

### 四、填充和步幅

一般来说，假设输入形状是nh×nw，卷积核窗口形状是kh×kw，那么输出形状将会是

<img src="https://edu-boker.oss-cn-beijing.aliyuncs.com/dl/6/2.png" style="zoom:50%;" />

所以卷积层的输出形状由输入形状和卷积核窗口形状决定。但是卷积层的两个超参数，即填充和步幅。它们可以对给定形状的输入和卷积核改变输出形状。

**填充：**

填充可以增加输出的高和宽。这常用来使输出与输入具有相同的高和宽。

填充（padding）是指在输入高和宽的两侧填充元素（通常是0元素）。下图里我们在原输入高和宽的两侧分别添加了值为0的元素，使得输入高和宽从3变成了5，并导致输出高和宽由2增加到4。图中的阴影部分为第一个输出元素及其计算所使用的输入和核数组元素：0×0+0×1+0×2+0×3=0。

<img src="https://edu-boker.oss-cn-beijing.aliyuncs.com/dl/6/3.png" style="zoom:50%;" />

一般来说，如果在高的两侧一共填充ph行，在宽的两侧一共填充pw列，那么输出形状将会是

<img src="https://edu-boker.oss-cn-beijing.aliyuncs.com/dl/6/4.png" style="zoom:50%;" />

也就是说，输出的高和宽会分别增加ph和pw。

在很多情况下，我们会设置ph=kh−1和pw=kw−1来使输入和输出具有相同的高和宽。这样会方便在构造网络时推测每个层的输出形状。假设这里kh是奇数，我们会在高的两侧分别填充ph/2行。如果kh是偶数，一种可能是在输入的顶端一侧填充⌈ph/2⌉行，而在底端一侧填充⌊ph/2⌋行。在宽的两侧填充同理。

卷积神经网络经常使用奇数高和宽的卷积核，如1、3、5和7，所以两端上的填充个数相等。

**步幅：**

步幅可以减小输出的高和宽，例如输出的高和宽仅为输入的高和宽的1/n（n为大于1的整数）。

对于二维互相关运算，卷积窗口从输入数组的最左上方开始，按从左往右、从上往下的顺序，依次在输入数组上滑动。我们将每次滑动的行数和列数称为步幅（stride）。

目前我们看到的例子里，在高和宽两个方向上步幅均为1。我们也可以使用更大步幅。下图展示了在高上步幅为3、在宽上步幅为2的二维互相关运算。可以看到，输出第一列第二个元素时，卷积窗口向下滑动了3行，而在输出第一行第二个元素时卷积窗口向右滑动了2列。当卷积窗口在输入上再向右滑动2列时，由于输入元素无法填满窗口，无结果输出。图中的阴影部分为输出元素及其计算所使用的输入和核数组元素：0×0+0×1+1×2+2×3=8、0×0+6×1+0×2+0×3=6。

<img src="https://edu-boker.oss-cn-beijing.aliyuncs.com/dl/6/5.png" style="zoom:50%;" />

一般来说，当高上步幅为sh，宽上步幅为sw时，输出形状为

<img src="https://edu-boker.oss-cn-beijing.aliyuncs.com/dl/6/6.png" style="zoom:50%;" />

如果设置ph=kh−1和pw=kw−1，那么输出形状将简化为⌊(nh+sh−1)/sh⌋×⌊(nw+sw−1)/sw⌋。更进一步，如果输入的高和宽能分别被高和宽上的步幅整除，那么输出形状将是(nh/sh)×(nw/sw)。

### 五、多输入通道和多输出通道

前面我们用到的输入和输出都是二维数组，但真实数据的维度经常更高。例如，彩色图像在高和宽2个维度外还有RGB（红、绿、蓝）3个颜色通道。假设彩色图像的高和宽分别是h和w（像素），那么它可以表示为一个3×h×w的多维数组。我们将大小为3的这一维称为通道（channel）维。我们将介绍含多个输入通道或多个输出通道的卷积核。

**多输入通道：**

当输入数据含多个通道时，我们需要构造一个输入通道数与输入数据的通道数相同的卷积核，从而能够与含多通道的输入数据做互相关运算。假设输入数据的通道数为ci，那么卷积核的输入通道数同样为ci。设卷积核窗口形状为kh×kw。当ci=1时，我们知道卷积核只包含一个形状为kh×kw的二维数组。当ci>1时，我们将会为每个输入通道各分配一个形状为kh×kw的核数组。把这ci个数组在输入通道维上连结，即得到一个形状为ci×kh×kw的卷积核。由于输入和卷积核各有ci个通道，我们可以在各个通道上对输入的二维数组和卷积核的二维核数组做互相关运算，再将这ci个互相关运算的二维输出按通道相加，得到一个二维数组。这就是含多个通道的输入数据与多输入通道的卷积核做二维互相关运算的输出。

下图展示了含2个输入通道的二维互相关计算的例子。在每个通道上，二维输入数组与二维核数组做互相关运算，再按通道相加即得到输出。下图中阴影部分为第一个输出元素及其计算所使用的输入和核数组元素：(1×1+2×2+4×3+5×4)+(0×0+1×1+3×2+4×3)=56。

<img src="https://edu-boker.oss-cn-beijing.aliyuncs.com/dl/6/7.png" style="zoom:50%;" />

**多输出通道：**

当输入通道有多个时，因为我们对各个通道的结果做了累加，所以不论输入通道数是多少，输出通道数总是为1。设卷积核输入通道数和输出通道数分别为ci和co，高和宽分别为kh和kw。如果希望得到含多个通道的输出，我们可以为每个输出通道分别创建形状为ci×kh×kw的核数组。将它们在输出通道维上连结，卷积核的形状即co×ci×kh×kw。在做互相关运算时，每个输出通道上的结果由卷积核在该输出通道上的核数组与整个输入数组计算而来。

### 六、1×1卷积层

卷积窗口形状为1×1（kh=kw=1）的多通道卷积层。我们通常称之为1×1卷积层，并将其中的卷积运算称为1×1卷积。因为使用了最小窗口，1×1卷积失去了卷积层可以识别高和宽维度上相邻元素构成的模式的功能。实际上，1×1卷积的主要计算发生在通道维上。下图展示了使用输入通道数为3、输出通道数为2的1×1卷积核的互相关计算。值得注意的是，输入和输出具有相同的高和宽。输出中的每个元素来自输入中在高和宽上相同位置的元素在不同通道之间的按权重累加。假设我们将通道维当作特征维，将高和宽维度上的元素当成数据样本，那么1×1卷积层的作用与全连接层等价。

<img src="https://edu-boker.oss-cn-beijing.aliyuncs.com/dl/6/8.png" style="zoom:50%;" />

在之后的模型里我们将会看到1×1卷积层被当作保持高和宽维度形状不变的全连接层使用。于是，我们可以通过调整网络层之间的通道数来控制模型复杂度。

1×1卷积层通常用来调整网络层之间的通道数，并控制模型复杂度.